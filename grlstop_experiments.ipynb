{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y00RfkU5gl3e"
      },
      "source": [
        "# pip install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZwmLEU0UAqy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ABdD1BElsnw"
      },
      "outputs": [],
      "source": [
        "tensorboard_log = '/xx/' # replace with your directory\n",
        "\n",
        "os.makedirs(tensorboard_log, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7I_gQZx_ND-"
      },
      "outputs": [],
      "source": [
        "\n",
        "df_all_targets = pd.DataFrame()\n",
        "\n",
        "model_name = 'PPO'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBfYaCjEgokt"
      },
      "outputs": [],
      "source": [
        "pip install gymnasium\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttq6_9qDgp-c"
      },
      "outputs": [],
      "source": [
        "pip install \"stable-baselines3[extra]>=2.0.0a4\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6x7QilC85No"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "\n",
        "from stable_baselines3 import PPO, A2C, DQN\n",
        "from stable_baselines3.common.env_util import make_vec_env, SubprocVecEnv, DummyVecEnv\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlZPhtptcf9v"
      },
      "source": [
        "## set reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfbdcxuNcker"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "os.environ['PYTHONASHSEED'] = '0'\n",
        "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
        "\n",
        "seed = 0\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEsAU3iFchK7"
      },
      "source": [
        "# utils functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNsfjE_5JQ-h"
      },
      "outputs": [],
      "source": [
        "#local modules directory for import\n",
        "DIR = '/xxhome/' # replace with utils home directory\n",
        "\n",
        "\n",
        "sys.path.append(DIR)\n",
        "\n",
        "\n",
        "# import utils fns\n",
        "from utils.read_data_fns import *\n",
        "from utils.eval_fns import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_all_targets = pd.DataFrame()\n",
        "DRL_DIR = '/xx/' # replace with working directory\n"
      ],
      "metadata": {
        "id": "iV2VUWqzvrIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcChrHeiJjDv"
      },
      "source": [
        "# clf functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z69DZFLa3S3p"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgZkVR22TjkR"
      },
      "source": [
        "# Callback functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVK3hivQH3eh"
      },
      "source": [
        "# TAREnv class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4m31be1WH3ej"
      },
      "outputs": [],
      "source": [
        "#keep track of topics\n",
        "ALL_VECTORS_PREDICTIONS_DIC = {}\n",
        "SELECTED_TOPICS_WITHOUT_TARGET = []\n",
        "ALL_VECORS_PREDICTIONS_DIC_EXIST = False\n",
        "\n",
        "SELECTED_TOPICS = [] # keep track of all randomly selected topics\n",
        "SELECTED_TOPICS_TARGET = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ekoUgp7H3ei"
      },
      "outputs": [],
      "source": [
        "# import RL env\n",
        "from rl_utils.ranking_utils import *\n",
        "from rl_utils.clf_utils import *\n",
        "from rl_utils.grlstop_tar_env import *\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5h89W_EyKUd8"
      },
      "source": [
        "# run experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1a26vbCk89NW"
      },
      "outputs": [],
      "source": [
        "TRAINING = True\n",
        "\n",
        "#penalise undershooting, tolerance with overshooting\n",
        "m = 4\n",
        "n = 1/4\n",
        "model_name = 'non_lin'\n",
        "\n",
        "#penalise overshooting, tolerance with undershooting\n",
        "m = 1/4\n",
        "n = 4\n",
        "model_name = 'non_lin_cost_obj'\n",
        "\n",
        "\n",
        "#blalanced\n",
        "m = 1\n",
        "n = 1\n",
        "model_name = 'lin_clf_lr'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "total_runs = 5 # if choose Deterministic = False (Stochatic) in predict()\n",
        "\n",
        "\n",
        "target_recalls = [1.0, 0.99, 0.95, 0.9, 0.85, 0.8, 0.75, 0.7]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVsh0NNiKUd9"
      },
      "outputs": [],
      "source": [
        "df_all_targets = pd.DataFrame()\n",
        "\n",
        "DRL_DIR = '/xx/' # replace with working directory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a-hmJB3KUd9"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwFBSDMUKUd-"
      },
      "outputs": [],
      "source": [
        "TRAINING = True\n",
        "topic_set = 'training'\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6lggvnCKUd-"
      },
      "outputs": [],
      "source": [
        "training_dataset = 'CLEF'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoAYq99JKUd-"
      },
      "source": [
        "#### sort topics by target location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NUS2o6YKUd_"
      },
      "outputs": [],
      "source": [
        "dataset_name = 'CLEF'\n",
        "\n",
        "qrels = \"/data/qrels/CLEF2017_qrels.txt\"\n",
        "\n",
        "\n",
        "qrel_fname, query_rel_dic = load_rel_data(qrels)\n",
        "\n",
        "run = \"/data/rankings/clef2017_training_ranking.txt\"\n",
        "\n",
        "doc_rank_dic, rank_rel_dic, rank_text_dic, rank_tfidf_dic = load_run_data_with_text_tfidf(run, dataset_name, topic_set)\n",
        "\n",
        "topics_list = make_topics_list(doc_rank_dic,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNR6TB0YKUd_"
      },
      "outputs": [],
      "source": [
        "#remove topic CD008760 last element, contains 64 items only, < 100 vector size\n",
        "topics_list= topics_list[:-1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQGN88mEKUeA"
      },
      "outputs": [],
      "source": [
        "topics_info = []\n",
        "\n",
        "for target_recall in target_recalls:\n",
        "  for t in topics_list:\n",
        "    topic_id, n_docs, n_rel, prev, target_location = load_topic_target_location(t,target_recall)\n",
        "    topic_id_target_recall = topic_id + \"_\" + str(target_recall)\n",
        "    print(topic_id, n_docs, n_rel, round(prev,3), target_location)\n",
        "    topics_info.append([topic_id, n_docs, n_rel, prev, target_recall, target_location, topic_id_target_recall])\n",
        "\n",
        "topics_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7IXYoJdKUeA"
      },
      "outputs": [],
      "source": [
        "\n",
        "df = pd.DataFrame(topics_info, columns=['topic_id', 'n_docs', 'n_rel', 'prev', 'target_recall', 'target_location', 'topic_id_target_recall'])\n",
        "df = df.sort_values(by=['target_recall', 'target_location'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0JivkKUKUeA"
      },
      "outputs": [],
      "source": [
        "sorted_target_loc_topics = list(df['topic_id_target_recall'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####load clf rel dic if exist"
      ],
      "metadata": {
        "id": "nRHt8h4JMgo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = 'CLEF'\n",
        "topic_set = 'training'\n",
        "\n",
        "# file name\n",
        "ALL_VECTORS_PREDICTIONS_DIC_file_name = dataset_name +'_'+ topic_set +'_'+ 'ALL_VECTORS_PREDICTIONS_DIC.pkl'\n",
        "ALL_VECTORS_PREDICTIONS_DIC_file_name = DRL_DIR+'data/'+ALL_VECTORS_PREDICTIONS_DIC_file_name\n",
        "\n",
        "# check if the file exists\n",
        "if not os.path.exists(ALL_VECTORS_PREDICTIONS_DIC_file_name):\n",
        "    # if doesn't exist, create an empty dictionary\n",
        "    ALL_VECTORS_PREDICTIONS_DIC = {}\n",
        "    SELECTED_TOPICS_WITHOUT_TARGET = []\n",
        "    ALL_VECTORS_PREDICTIONS_DIC_EXIST = False\n",
        "\n",
        "else:\n",
        "    # if exists, load the dictionary from the file\n",
        "    with open(ALL_VECTORS_PREDICTIONS_DIC_file_name, \"rb\") as f:\n",
        "        ALL_VECTORS_PREDICTIONS_DIC = pickle.load(f)\n",
        "\n",
        "    SELECTED_TOPICS_WITHOUT_TARGET = list(ALL_VECTORS_PREDICTIONS_DIC.keys())\n",
        "    ALL_VECTORS_PREDICTIONS_DIC_EXIST = True\n"
      ],
      "metadata": {
        "id": "agtOQA-uMSIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUBK7wAeKUeB"
      },
      "source": [
        "####ordered topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yr2MsiaIKUeB"
      },
      "outputs": [],
      "source": [
        "TRAINING = True\n",
        "\n",
        "SELECTED_TOPICS_ORDERERD = sorted_target_loc_topics\n",
        "SELECTED_TOPICS_ORDERERD_INDEX = 0\n",
        "\n",
        "SELECTED_TOPICS_TARGET = target_recalls\n",
        "SELECTED_TOPICS_TARGET_INDEX = 0\n",
        "\n",
        "# Instantiate the vec env\n",
        "\n",
        "#random topic selection for each env instance\n",
        "SELECTED_TOPICS = [] # reset before/after each call, keep track of all randomly selected topics\n",
        "SELECTED_TOPICS_TARGET = []\n",
        "\n",
        "n_envs=len(sorted_target_loc_topics)\n",
        "vec_env = make_vec_env(TAREnv, n_envs=len(sorted_target_loc_topics), env_kwargs=dict(target_recall=None, topics_list = sorted_target_loc_topics, topic_id=None, size=100, render_mode='human'))\n",
        "\n",
        "SELECTED_TOPICS = [] # reset before/after each call, keep track of all randomly selected topics\n",
        "SELECTED_TOPICS_TARGET = []\n",
        "\n",
        "train_size = len(topics_list)*len(target_recalls)\n",
        "\n",
        "\n",
        "vec_env_train = vec_env\n",
        "\n",
        "#save dic for first time\n",
        "if not ALL_VECTORS_PREDICTIONS_DIC_EXIST:\n",
        "    # Save the dictionary to a file\n",
        "    with open(ALL_VECTORS_PREDICTIONS_DIC_file_name, 'wb') as f:\n",
        "        pickle.dump(ALL_VECTORS_PREDICTIONS_DIC, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNSy3lVoKUeB"
      },
      "source": [
        "#### Hyperparameter Settings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def nearest_factor_of(target, total):\n",
        "    # find all factors of `total`\n",
        "    factors = [i for i in range(1, total + 1) if total % i == 0]\n",
        "\n",
        "    # find the factor closest to the target\n",
        "    closest_factor = min(factors, key=lambda x: abs(x - target))\n",
        "    return closest_factor\n",
        "\n"
      ],
      "metadata": {
        "id": "wIYyQb3HH2p4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5pu01e9KUeB"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Train the agent\n",
        "\n",
        "n_envs=len(sorted_target_loc_topics)\n",
        "\n",
        "\n",
        "learning_rate_initial = 0.0003\n",
        "\n",
        "\n",
        "learning_rate = learning_rate_initial\n",
        "learning_rate_type = '_lr_static'\n",
        "\n",
        "\n",
        "ent_coef = 0.1\n",
        "\n",
        "gamma = 0.99\n",
        "gae_lambda = 0.98\n",
        "clip_range=0.1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "n_epochs =10\n",
        "\n",
        "n_steps = 10\n",
        "\n",
        "\n",
        "\n",
        "total_rollout = n_steps * n_envs  # Total rollout buffer size\n",
        "target_mini_batch = int(1/4 * total_rollout)  # Target value (1/4 of total)\n",
        "\n",
        "\n",
        "# get the nearest factor\n",
        "batch_size = nearest_factor_of(total_rollout, target_mini_batch)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "nn_nodes = 64\n",
        "\n",
        "policy_kwargs = dict(\n",
        "    net_arch=[nn_nodes, nn_nodes]  # Two hidden layers with nn_nodes\n",
        ")\n",
        "\n",
        "\n",
        "total_timesteps = 20_000_000\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3if69onoKUeC"
      },
      "source": [
        "####  PPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BT6P-YGiKUeD"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "tb_log_name = model_name+\"_\"+training_dataset+\"_nstps\"+str(n_steps)+\"_btch\"+str(batch_size)+\"_ts\"+str(total_timesteps)+ \"_ent\"+str(ent_coef)+\"_epch\"+str(n_epochs)+learning_rate_type+str(learning_rate_initial)+\"_clip\"+str(clip_range)+\"_nn\"+str(nn_nodes) +\"_target\"+str(target_recall)\n",
        "\n",
        "\n",
        "callback_log = tensorboard_log+'running/'\n",
        "# Checkpoint callback to save the model periodically\n",
        "checkpoint_callback = CheckpointCallback(save_freq=10000, save_path=callback_log, name_prefix=tb_log_name)\n",
        "\n",
        "\n",
        "\n",
        "# patience is set to patience steps\n",
        "patience = 10*n_steps*n_envs #as a general early stopping rule, allow for 10 rollouts w/o improvement more than 1%\n",
        "\n",
        "early_stopping_callback = EarlyStoppingCallback(patience=patience, min_delta=0.001, verbose=1)\n",
        "\n",
        "\n",
        "\n",
        "# Combine the callbacks\n",
        "callbacks = [checkpoint_callback, early_stopping_callback]\n",
        "\n",
        "model = PPO(\n",
        "    policy = 'MlpPolicy',\n",
        "    env = vec_env_train,\n",
        "    n_steps = n_steps,\n",
        "    batch_size = batch_size,\n",
        "    n_epochs = n_epochs,\n",
        "    gamma = gamma,\n",
        "    gae_lambda = gae_lambda,\n",
        "    ent_coef = ent_coef,\n",
        "    clip_range = clip_range,\n",
        "    verbose=0,\n",
        "    learning_rate = learning_rate,\n",
        "    policy_kwargs=policy_kwargs,\n",
        "    seed=0,\n",
        "    tensorboard_log= tensorboard_log)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model.learn(total_timesteps=total_timesteps, tb_log_name=tb_log_name, callback=callbacks)\n",
        "\n",
        "model.save(tensorboard_log+'model_'+tb_log_name)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPUQjmYvKUeN"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir \"$tensorboard_log\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knQiPHlAKUeP"
      },
      "source": [
        "## TESTING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNRNVPMRKUeQ"
      },
      "source": [
        "### Load Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Wr4ctQKKUeQ"
      },
      "outputs": [],
      "source": [
        "TRAINING = False\n",
        "\n",
        "\n",
        "model_load_dir = tensorboard_log + 'model_'+tb_log_name\n",
        "\n",
        "\n",
        "model = PPO.load(model_load_dir, env=vec_env_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEpPCVykKUeR"
      },
      "source": [
        "### all targets & datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y75xOyusKUeR"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "dataset_names = [\"CLEF2017\", \"CLEF2018\", \"CLEF2019\"]\n",
        "\n",
        "qrels_dic = {\n",
        "    \"CLEF2017\" : \"data/qrels/CLEF2017_qrels.txt\",\n",
        "    \"CLEF2018\" : \"data/qrels/CLEF2018_qrels.txt\",\n",
        "    \"CLEF2019\" : \"data/qrels/CLEF2019_qrels.txt\",\n",
        "}\n",
        "rankings_dic = {\n",
        "    \"CLEF2017\" : \"data/rankings/clef2017_ranking.txt\",\n",
        "    \"CLEF2018\" : \"data/rankings/clef2018_ranking.txt\",\n",
        "    \"CLEF2019\" : \"data/rankings/clef2019_ranking.txt\",\n",
        "}\n",
        "\n",
        "\n",
        "target_recalls = [1.0, 0.9, 0.8, 0.7]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ek-Byzl6KUeR"
      },
      "source": [
        "### results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w56U3Ey1KUeR"
      },
      "outputs": [],
      "source": [
        "TRAINING = False\n",
        "\n",
        "total_runs = 1\n",
        "topic_set = 'test'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### all targets & datasets\n",
        "for dataset_name in dataset_names:\n",
        "\n",
        "  # file name\n",
        "  ALL_VECTORS_PREDICTIONS_DIC_file_name = dataset_name +'_'+ topic_set +'_'+ 'ALL_VECTORS_PREDICTIONS_DIC.pkl'\n",
        "  ALL_VECTORS_PREDICTIONS_DIC_file_name = DRL_DIR+'data/'+ALL_VECTORS_PREDICTIONS_DIC_file_name\n",
        "\n",
        "  # check if the file exists\n",
        "  if not os.path.exists(ALL_VECTORS_PREDICTIONS_DIC_file_name):\n",
        "    # if doesn't exist, create an empty dictionary\n",
        "    ALL_VECTORS_PREDICTIONS_DIC = {}\n",
        "    SELECTED_TOPICS_WITHOUT_TARGET = []\n",
        "    ALL_VECTORS_PREDICTIONS_DIC_EXIST = False\n",
        "  else:\n",
        "    # if exists, load the dictionary from the file\n",
        "    with open(ALL_VECTORS_PREDICTIONS_DIC_file_name, \"rb\") as f:\n",
        "      ALL_VECTORS_PREDICTIONS_DIC = pickle.load(f)\n",
        "      SELECTED_TOPICS_WITHOUT_TARGET = list(ALL_VECTORS_PREDICTIONS_DIC.keys())\n",
        "      ALL_VECTORS_PREDICTIONS_DIC_EXIST = True\n",
        "\n",
        "  qrels = qrels_dic[dataset_name]\n",
        "\n",
        "  qrel_fname, query_rel_dic = load_rel_data(qrels)\n",
        "\n",
        "  run = rankings_dic[dataset_name]\n",
        "  #doc_rank_dic, rank_rel_dic = load_run_data(run)\n",
        "  doc_rank_dic, rank_rel_dic, rank_text_dic, rank_tfidf_dic = load_run_data_with_text_tfidf(run, dataset_name, topic_set)\n",
        "\n",
        "  topics_list = make_topics_list(doc_rank_dic,1)  # sort topics by no docs\n",
        "\n",
        "  if dataset_name == 'CLEF2019':\n",
        "      #remove topic CD012164 last element, contains 61 items only, < 100 vector size\n",
        "      topics_list= topics_list[:-1]\n",
        "\n",
        "  for target_recall in target_recalls:\n",
        "\n",
        "    # Instantiate the vec env\n",
        "\n",
        "    #random topic selection for each env instance\n",
        "    SELECTED_TOPICS = [] # reset before/after each call, keep track of all randomly selected topics\n",
        "\n",
        "    vec_env = make_vec_env(TAREnv, n_envs=len(topics_list), env_kwargs=dict(target_recall=target_recall, topics_list = topics_list, topic_id=None, size=100, render_mode='human'))\n",
        "\n",
        "    SELECTED_TOPICS = [] # reset before/after each call, keep track of all randomly selected topics\n",
        "\n",
        "    test_size = len(topics_list)\n",
        "    vec_env_test = vec_env\n",
        "\n",
        "    df = pd.DataFrame()\n",
        "    df_all_runs = pd.DataFrame()\n",
        "\n",
        "    for run in range(total_runs):\n",
        "\n",
        "      # Test the trained agent\n",
        "      # using the vecenv\n",
        "      vec_env_test = vec_env\n",
        "      obs = vec_env_test.reset()\n",
        "      test_steps = 100\n",
        "      vector_size = 100\n",
        "\n",
        "\n",
        "      n_env = test_size\n",
        "      agent=0\n",
        "      target=0\n",
        "      agent_vector=[]\n",
        "      terminal_observation=[]\n",
        "\n",
        "      topics = []\n",
        "      recalls = []\n",
        "      costs=[]\n",
        "      e_costs = []\n",
        "      reliabilities = []\n",
        "      rewards = []\n",
        "      distances = []\n",
        "      differences = []\n",
        "      targets = []\n",
        "      run_cnts = []\n",
        "\n",
        "      for eID in range(test_size):\n",
        "        print(f\"=================== env {eID}\")\n",
        "        env = vec_env_test.envs[eID]\n",
        "        obs, info = env.reset()\n",
        "\n",
        "        for step in range(test_steps):\n",
        "\n",
        "          action, _ = model.predict(obs, deterministic=True) # predict all next steps\n",
        "\n",
        "          obs, reward, done, trun,info = env.step(action)\n",
        "\n",
        "          if done or trun:\n",
        "                      topic_id = info['topic_id']\n",
        "                      recall = info['recall']\n",
        "                      cost = info['cost']\n",
        "                      e_cost =  info['e_cost']\n",
        "\n",
        "                      distance = info['distance']\n",
        "\n",
        "                      agent = info['agent']\n",
        "                      target = info['target']\n",
        "                      agent_vector = info['agent_vector']\n",
        "                      terminal_observation = info['terminal_observation']\n",
        "\n",
        "                      difference = target_recall - recall\n",
        "\n",
        "\n",
        "\n",
        "                      reliability = 1 if recall >= target_recall else 0\n",
        "                      topics.append(topic_id)\n",
        "                      recalls.append(recall)\n",
        "                      costs.append(cost)\n",
        "                      e_costs.append(e_cost)\n",
        "                      reliabilities.append(reliability)\n",
        "                      rewards.append(reward)\n",
        "                      distances.append(distance)\n",
        "                      targets.append(target)\n",
        "                      run_cnts.append(run)\n",
        "                      differences.append(difference)\n",
        "\n",
        "                      df_tmp = pd.DataFrame( list(zip([dataset_name]*len(topics_list), topics, run_cnts, recalls, reliabilities, costs, e_costs, rewards, differences, distances, targets)),\n",
        "                      columns =['Dataset', 'Topic', 'Run', 'Recall', 'Reliability', 'Cost', 'e-Cost', 'Reward', 'Difference', 'Distance', 'Target'])\n",
        "\n",
        "\n",
        "                      df = pd.concat([df_tmp])\n",
        "\n",
        "\n",
        "                      df.groupby('Topic').mean(numeric_only=True)\n",
        "\n",
        "                      break\n",
        "\n",
        "\n",
        "\n",
        "      df_all_runs = pd.concat([df_all_runs, df])\n",
        "\n",
        "\n",
        "\n",
        "    df_all_runs['Model'] = model_name\n",
        "    df_all_runs['Model_settings'] = tb_log_name\n",
        "    df_all_runs['Target_Recall'] = target_recall\n",
        "\n",
        "\n",
        "\n",
        "    df_all_targets = pd.concat([df_all_targets, df_all_runs], ignore_index = True)\n",
        "\n",
        "    #save dic for first time\n",
        "    if not ALL_VECTORS_PREDICTIONS_DIC_EXIST:\n",
        "        # Save the dictionary to a file\n",
        "        with open(ALL_VECTORS_PREDICTIONS_DIC_file_name, 'wb') as f:\n",
        "            pickle.dump(ALL_VECTORS_PREDICTIONS_DIC, f)\n",
        "\n",
        "\n",
        "display(df_all_targets)\n",
        "\n",
        "display(df_all_targets.describe())\n",
        "\n",
        "display(df_all_targets.groupby(['Target_Recall','Dataset']).mean(numeric_only=True).round(3))\n",
        "display(df_all_targets.groupby(['Target_Recall','Dataset']).std(numeric_only=True).round(3))\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "6OfYu6PpKUeT",
        "h3kTyNxhKUer",
        "XucMp1jrKUer",
        "L_m3PgsAKUet",
        "-vYKTE9JKUet",
        "zJ9MctX2Id3t",
        "9dFJ5oSOPBw6",
        "o5r9nM3LId3v",
        "UO_h3GI2Iwbu",
        "BibKjQzxIwbw",
        "jjqCriWRIwbx",
        "CBZDmJKnIwbx",
        "1-rMZja54cf8",
        "gzUHZper4cf-",
        "9zGX1wEh4cgA",
        "WZHUvlLw4cgB",
        "iaeqAfb1Eks6",
        "k9k5FPM74cgC",
        "pgAe_YCx4cgD",
        "c2qi2ykqoLNR",
        "LfBciPy_oLNR",
        "7jUmSq_qoLNZ"
      ],
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}